{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Union\n",
    "from calendar import c\n",
    "from typing import Literal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_hat: pd.Series, y: pd.Series):\n",
    "    assert y_hat.size == y.size\n",
    "\n",
    "    correct_predictions = sum(a == p for a, p in zip(y, y_hat))\n",
    "    total_predictions = len(y)\n",
    "    \n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy\n",
    "    \n",
    "\n",
    "def precision(y_hat: pd.Series, y: pd.Series, cls: Union[int, str]):\n",
    "    true_positives=y_hat[y == cls].value_counts().get(cls,0)\n",
    "    total_prediction=len([y_hat == cls])\n",
    "\n",
    "    # print(y_hat)\n",
    "    # print(\"true_postivi\" , true_positives)\n",
    "\n",
    "    if total_prediction == 0:\n",
    "        return 0.0  # Avoid division by zero\n",
    "\n",
    "    precision = true_positives / total_prediction\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_hat: pd.Series, y: pd.Series, cls: Union[int, str]):\n",
    "    true_positives=y_hat[y == cls].value_counts().get(cls,0)\n",
    "    total_actual=len([y == cls])\n",
    "\n",
    "    if total_actual == 0:\n",
    "        return 0.0\n",
    "    recall = true_positives / total_actual\n",
    "    return recall\n",
    "\n",
    "\n",
    "def rmse(y_hat: pd.Series, y: pd.Series):\n",
    "    assert ValueError(\"Input lists must have the same length.\")\n",
    "\n",
    "    squared_errors = pd.sub(y, y_hat) ** 2 \n",
    "    mean_squared_error = squared_errors.mean()\n",
    "    rmse = np.sqrt(mean_squared_error)\n",
    "    return rmse\n",
    "\n",
    "\n",
    "def mae(y_hat: pd.Series, y: pd.Series):\n",
    "    assert ValueError(\"Input lists must have the same length.\")\n",
    "\n",
    "    squared_errors = pd.sub(y, y_hat) ** 2 \n",
    "    return squared_errors.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_ifreal(y: pd.Series):\n",
    "    if y.dtype.name == \"category\":\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "#calculated on target variable\n",
    "def entropy(Y: pd.Series):\n",
    "    unique_class, class_counts = np.unique(Y, return_counts=True)\n",
    "    probability = class_counts/len(Y)\n",
    "    return -np.sum(probability * np.log2(probability))\n",
    "\n",
    "def gini_index(y):\n",
    "    unique_class, class_counts = np.unique(y, return_counts=True)\n",
    "    probability = class_counts/len(y)\n",
    "    return 1-np.sum(probability ** 2)\n",
    "\n",
    "def MSE():\n",
    "    pass\n",
    "\n",
    "def information_gain(attr: pd.DataFrame, Y: pd.Series, feature_idx):\n",
    "    # conataining unique value of particular feature\n",
    "    unique_classs = np.unique(attr[feature_idx])\n",
    "    node_entropy=0\n",
    "    total_child_entropy = 0\n",
    "\n",
    "    #entropy on whole dataset target label\n",
    "    node_entropy = entropy(Y)\n",
    "\n",
    "    for value in unique_classs:\n",
    "        # taking sub part of target which has same value for the feature\n",
    "        child_target = Y[attr[feature_idx] == value]\n",
    "        total_child_entropy += len(child_target) / \\\n",
    "            len(Y) * entropy(child_target)\n",
    "  \n",
    "    return node_entropy-total_child_entropy\n",
    "\n",
    "\n",
    "def opt_split_attribute(X: pd.DataFrame, y: pd.Series, criterion=\"information_gain\"):\n",
    "    \"\"\"\n",
    "    Function to find the optimal attribute to split about.\n",
    "    If needed you can split this function into 2, one for discrete and one for real valued features.\n",
    "    You can also change the parameters of this function according to your implementation.\n",
    "\n",
    "    features: pd.Series is a list of all the attributes we have to split upon\n",
    "\n",
    "    return: attribute to split upon\n",
    "    \"\"\"\n",
    "\n",
    "    # According to wheather the features are real or discrete valued and the criterion, find the attribute\n",
    "    #  from the features series with the maximum information gain\n",
    "    #  (entropy or varinace based on the type of output) or minimum gini index (discrete output).\n",
    "\n",
    "    best_feature=0\n",
    "\n",
    "    if criterion ==\"information_gain\":\n",
    "        max_info_gain=0\n",
    "        for column_name in X.columns:\n",
    "            new_info_gain=information_gain(X,y,column_name)\n",
    "            if new_info_gain > max_info_gain:\n",
    "                max_info_gain=new_info_gain\n",
    "                best_feature=column_name\n",
    "    else :\n",
    "        gini_value=float('inf')\n",
    "        for column_name in X.columns:\n",
    "            new_gini=gini_index(X[column_name])\n",
    "            if new_gini < gini_value:\n",
    "                gini_value=new_gini\n",
    "                best_feature=column_name\n",
    "    \n",
    "    return best_feature\n",
    "\n",
    "\n",
    "def split_data(X: pd.DataFrame, y: pd.Series, attribute, value):\n",
    "    \"\"\"\n",
    "    Funtion to split the data according to an attribute.\n",
    "    If needed you can split this function into 2, one for discrete and one for real valued features.\n",
    "    You can also change the parameters of this function according to your implementation.\n",
    "\n",
    "    attribute: attribute/feature to split upon\n",
    "    value: value of that attribute to split upon\n",
    "\n",
    "    return: splitted data(Input and output)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Split the data based on a particular value of a particular attribute. \n",
    "    # You may use masking as a tool to split the data.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, data=None, feature_value=None,target=None,child=None,result=None):\n",
    "        self.data = data  # data corresponding to the node [matrix]\n",
    "        self.target = target  # y data\n",
    "        self.children = child  # child names & objects\n",
    "        self.feature = feature_value  # value at node\n",
    "        self.result = result\n",
    "\n",
    "    def plot(self, level=0):\n",
    "        indent = \"  \" * level\n",
    "        print(f\"{indent}|- {self.feature}: {self.result}\")\n",
    "        if self.children is not None:\n",
    "            for child in self.children:\n",
    "                self.children[child].plot(level + 1)\n",
    "\n",
    "class DecisionTree:\n",
    "    # criterion won't be used for regression\n",
    "    criterion: Literal[\"information_gain\", \"gini_index\"]\n",
    "    max_depth: int  # The maximum depth the tree can grow to\n",
    "    root = None\n",
    "\n",
    "    def __init__(self, criterion, max_depth=5):\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    # def fit_DI_DO(self, data_frame: pd.DataFrame, target: pd.Series, depth=0,criterion=\"information_gain\"):\n",
    "    #     return self.id3(data_frame,target,depth)\n",
    "\n",
    "    def id3(self,data_frame: pd.DataFrame, target: pd.Series, depth=0,criterion=\"information_gain\"):\n",
    "            # If all instances have the same target value, create a leaf node\n",
    "            if len(np.unique(target)) == 1:\n",
    "                majority_class = target.mode()[0]\n",
    "                return Node(result=majority_class)\n",
    "\n",
    "            # If there are no more features to split on, create a leaf node with the majority class\n",
    "            if len(list(data_frame.columns)) == 0:\n",
    "                return Node(result=majority_class)\n",
    "\n",
    "            # If maximum depth is reached, create a leaf node with the majority class\n",
    "            if self.max_depth is not None and depth == self.max_depth:\n",
    "                majority_class = target.mode()[0]\n",
    "                return Node(result=majority_class)\n",
    "\n",
    "            # Choose the best feature to split on based on information gain considering only single level in tree\n",
    "            best_attribute=opt_split_attribute(data_frame,target,criterion=criterion)\n",
    "            root=Node(data=data_frame,target=target,feature_value=best_attribute,child={})\n",
    "\n",
    "            for value in np.unique(data_frame[best_attribute]):\n",
    "                subset_idx=data_frame[best_attribute] == value\n",
    "                subset_data = data_frame[subset_idx]\n",
    "                subset_target = target[subset_idx]\n",
    "                if len(subset_target) > 0 :\n",
    "                    root.children[value]=self.id3(subset_data, subset_target)\n",
    "                \n",
    "            return root\n",
    "\n",
    "    def predict_multiway(self,tree, sample):\n",
    "        # Reached a leaf node which contains the result value, return the result\n",
    "        if tree.result is not None:\n",
    "            return tree.result\n",
    "\n",
    "        # Not a leaf node, continue traversing the tree\n",
    "        feature_value = sample[tree.feature]\n",
    "\n",
    "        if feature_value in tree.children:\n",
    "            child_node = tree.children[feature_value]\n",
    "            return self.predict_multiway(child_node, sample)\n",
    "        else:\n",
    "            # Handle missing values or unknown values as needed\n",
    "            return None\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series):\n",
    "        return self.id3(X,y,depth=0)     \n",
    "\n",
    "    def predict(self, tree,X):\n",
    "        return self.predict_multiway(tree,X)\n",
    "\n",
    "    def plot(self):\n",
    "        \"\"\"\n",
    "        Function to plot the tree\n",
    "\n",
    "        Output Example:\n",
    "        ?(X1 > 4)\n",
    "            Y: ?(X2 > 7)\n",
    "                Y: Class A\n",
    "                N: Class B\n",
    "            N: Class C\n",
    "        Where Y => Yes and N => No\n",
    "        \"\"\"\n",
    "        if self.root is not None:\n",
    "            self.root.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|- 3: None\n",
      "  |- 0: None\n",
      "    |- None: 3\n",
      "    |- None: 0\n",
      "    |- None: 0\n",
      "    |- 1: None\n",
      "      |- None: 2\n",
      "      |- None: 4\n",
      "    |- 1: None\n",
      "      |- None: 2\n",
      "      |- None: 2\n",
      "      |- None: 0\n",
      "  |- 1: None\n",
      "    |- None: 0\n",
      "    |- None: 0\n",
      "    |- None: 2\n",
      "    |- None: 4\n",
      "  |- 0: None\n",
      "    |- 1: None\n",
      "      |- None: 0\n",
      "      |- None: 3\n",
      "    |- None: 0\n",
      "    |- None: 0\n",
      "    |- 1: None\n",
      "      |- None: 3\n",
      "      |- None: 0\n",
      "      |- None: 3\n",
      "    |- None: 0\n",
      "  |- 2: None\n",
      "    |- None: 2\n",
      "    |- None: 2\n",
      "    |- None: 4\n",
      "  |- 1: None\n",
      "    |- None: 1\n",
      "    |- None: 3\n",
      "    |- None: 1\n",
      "Criteria : information_gain\n",
      "Accuracy:  1.0\n",
      "Precision:  13.0\n",
      "Recall:  13.0\n",
      "Precision:  6.0\n",
      "Recall:  6.0\n",
      "Precision:  5.0\n",
      "Recall:  5.0\n",
      "Precision:  3.0\n",
      "Recall:  3.0\n",
      "Precision:  3.0\n",
      "Recall:  3.0\n",
      "|- 3: None\n",
      "  |- 0: None\n",
      "    |- None: 3\n",
      "    |- None: 0\n",
      "    |- None: 0\n",
      "    |- 1: None\n",
      "      |- None: 2\n",
      "      |- None: 4\n",
      "    |- 1: None\n",
      "      |- None: 2\n",
      "      |- None: 2\n",
      "      |- None: 0\n",
      "  |- 1: None\n",
      "    |- None: 0\n",
      "    |- None: 0\n",
      "    |- None: 2\n",
      "    |- None: 4\n",
      "  |- 0: None\n",
      "    |- 1: None\n",
      "      |- None: 0\n",
      "      |- None: 3\n",
      "    |- None: 0\n",
      "    |- None: 0\n",
      "    |- 1: None\n",
      "      |- None: 3\n",
      "      |- None: 0\n",
      "      |- None: 3\n",
      "    |- None: 0\n",
      "  |- 2: None\n",
      "    |- None: 2\n",
      "    |- None: 2\n",
      "    |- None: 4\n",
      "  |- 1: None\n",
      "    |- None: 1\n",
      "    |- None: 3\n",
      "    |- None: 1\n",
      "Criteria : gini_index\n",
      "Accuracy:  1.0\n",
      "Precision:  13.0\n",
      "Recall:  13.0\n",
      "Precision:  6.0\n",
      "Recall:  6.0\n",
      "Precision:  5.0\n",
      "Recall:  5.0\n",
      "Precision:  3.0\n",
      "Recall:  3.0\n",
      "Precision:  3.0\n",
      "Recall:  3.0\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "N = 30\n",
    "P = 5\n",
    "X = pd.DataFrame({i: pd.Series(np.random.randint(P, size=N), dtype=\"category\") for i in range(5)})\n",
    "y = pd.Series(np.random.randint(P, size=N), dtype=\"category\")\n",
    "\n",
    "\n",
    "for criteria in [\"information_gain\", \"gini_index\"]:\n",
    "    tree = DecisionTree(criterion=criteria)  # Split based on Inf. Gain\n",
    "    root=tree.fit(X, y)\n",
    "    y_hat=[]\n",
    "    for idx,x in X.iterrows():\n",
    "        y_hat.append(tree.predict(root,x))\n",
    "    \n",
    "    # print(y_hat)\n",
    "    y_hat_series=pd.Series(y_hat)\n",
    "    # print(y_hat_series)\n",
    "\n",
    "    # plt.plot(range(30),y,marker='o')\n",
    "    # plt.plot(range(30),y_hat_series)\n",
    "    # plt.show()\n",
    "\n",
    "    root.plot()\n",
    "    print(\"Criteria :\", criteria)\n",
    "    print(\"Accuracy: \", accuracy(y_hat_series, y))\n",
    "    for cls in y.unique():\n",
    "        print(\"Precision: \", precision(y_hat_series, y, cls))\n",
    "        print(\"Recall: \", recall(y_hat_series, y, cls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr=DecisionTree(criterion=\"information_gain\")\n",
    "# root=tr.fit(X,y)\n",
    "# y_hat=[]\n",
    "# for idx,x in X.iterrows():\n",
    "#     y_hat.append(tr.predict_multiway(root,x))\n",
    "# print(y_hat)\n",
    "# y_hat_series=pd.Series(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(range(30),y,y_hat,marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3de59ade2ee4eabf8d6554090db31bdd94608df00f05391c2d316a7da62ee3f6"
  },
  "kernelspec": {
   "display_name": "Python 3.10.9 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
