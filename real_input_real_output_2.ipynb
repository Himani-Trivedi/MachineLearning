{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from metrics import *\n",
    "from tree.base import DecisionTree,Node\n",
    "\n",
    "from calendar import c\n",
    "from typing import Literal\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_hat: pd.Series, y: pd.Series):\n",
    "    assert y_hat.size == y.size\n",
    "\n",
    "    correct_predictions = sum(a == p for a, p in zip(y, y_hat))\n",
    "    total_predictions = len(y)\n",
    "    \n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy\n",
    "    \n",
    "\n",
    "def precision(y_hat: pd.Series, y: pd.Series, cls: Union[int, str]):\n",
    "    true_positives=y_hat[y == cls].value_counts().get(cls,0)\n",
    "    total_prediction=len([y_hat == cls])\n",
    "\n",
    "    print(y_hat)\n",
    "    print(\"true_postivi\" , true_positives)\n",
    "\n",
    "    if total_prediction == 0:\n",
    "        return 0.0  # Avoid division by zero\n",
    "\n",
    "    precision = true_positives / total_prediction\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_hat: pd.Series, y: pd.Series, cls: Union[int, str]):\n",
    "    true_positives=y_hat[y == cls].value_counts().get(cls,0)\n",
    "    total_actual=len([y == cls])\n",
    "\n",
    "    if total_actual == 0:\n",
    "        return 0.0\n",
    "    recall = true_positives / total_actual\n",
    "    return recall\n",
    "\n",
    "\n",
    "def rmse(y_hat: pd.Series, y: pd.Series):\n",
    "    assert ValueError(\"Input lists must have the same length.\")\n",
    "\n",
    "    squared_errors = (y - y_hat) ** 2 \n",
    "    mean_squared_error = squared_errors.mean()\n",
    "    rmse = np.sqrt(mean_squared_error)\n",
    "    return rmse\n",
    "\n",
    "\n",
    "def mae(y_hat: pd.Series, y: pd.Series):\n",
    "    assert ValueError(\"Input lists must have the same length.\")\n",
    "\n",
    "    squared_errors = (y - y_hat) ** 2 \n",
    "    return squared_errors.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_ifreal(y: pd.Series):\n",
    "    if y.dtype.name == \"category\":\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def entropy(Y: pd.Series):\n",
    "    unique_class, class_counts = np.unique(Y, return_counts=True)\n",
    "    probability = class_counts/len(Y)\n",
    "    return -np.sum(probability * np.log2(probability))\n",
    "\n",
    "def gini_index(Y: pd.Series):\n",
    "    unique_class, class_counts = np.unique(Y, return_counts=True)\n",
    "    probability = class_counts/len(Y)\n",
    "    return 1-np.sum(probability ** 2)\n",
    "\n",
    "def MSE(target : pd.Series,mean):\n",
    "    squared_diff = (target - mean).apply(lambda x: x**2)\n",
    "    mse = squared_diff.mean()\n",
    "    return mse\n",
    "\n",
    "\n",
    "#returning the minimum loss from the splits of the feature\n",
    "def information_gain(attr: pd.DataFrame, Y: pd.Series, feature_idx):\n",
    "    loss=float('inf')\n",
    "    final_thresholder=0\n",
    "    threshold=0\n",
    "    for i in range(2,X.shape[0]):\n",
    "        threshold=(X[feature_idx][i-1] + X[feature_idx][i])/2\n",
    "        \n",
    "        r1_x=X[X[feature_idx] < threshold]\n",
    "        r1_y=Y[X[feature_idx] < threshold]\n",
    "\n",
    "        r2_x=X[X[feature_idx] > threshold]\n",
    "        r2_y=Y[X[feature_idx] > threshold]\n",
    "\n",
    "        # print(\"hello\")\n",
    "        # print(Y[X[feature_idx] < threshold])\n",
    "\n",
    "        new_loss=MSE(r1_y,r1_y.mean())/len(r1_x) + \\\n",
    "                MSE(r2_y, r2_y.mean()) /len(r2_x)\n",
    "\n",
    "        print(\"loss\" , new_loss)\n",
    "        if new_loss < loss:\n",
    "            loss=new_loss\n",
    "            final_thresholder=threshold\n",
    "        # break\n",
    "    return  [final_thresholder,loss]\n",
    "\n",
    "\n",
    "def opt_split_attribute(X: pd.DataFrame, Y: pd.Series, criterion=\"information_gain\"):\n",
    "    \"\"\"\n",
    "    Function to find the optimal attribute to split about.\n",
    "    If needed you can split this function into 2, one for discrete and one for real valued features.\n",
    "    You can also change the parameters of this function according to your implementation.\n",
    "\n",
    "    features: pd.Series is a list of all the attributes we have to split upon\n",
    "\n",
    "    return: attribute to split upon\n",
    "    \"\"\"\n",
    "\n",
    "    # According to wheather the features are real or discrete valued and the criterion, find the attribute\n",
    "    #  from the features series with the maximum information gain\n",
    "    #  (entropy or varinace based on the type of output) or minimum gini index (discrete output).\n",
    "\n",
    "    if criterion == \"information_gain\":\n",
    "        max_info_gain=0\n",
    "        best_feature=0\n",
    "        threshold=0\n",
    "        for column_name in X.columns:\n",
    "            X.sort_values(by=column_name)\n",
    "            new_info_gain=information_gain(X,Y,column_name)\n",
    "            if new_info_gain[1] < max_info_gain:\n",
    "                max_info_gain=new_info_gain\n",
    "                best_feature=column_name\n",
    "                threshold=new_info_gain[0]\n",
    "\n",
    "    return [best_feature,threshold]\n",
    "\n",
    "\n",
    "def split_data(X: pd.DataFrame, y: pd.Series, attribute, value):\n",
    "    \"\"\"\n",
    "    Funtion to split the data according to an attribute.\n",
    "    If needed you can split this function into 2, one for discrete and one for real valued features.\n",
    "    You can also change the parameters of this function according to your implementation.\n",
    "\n",
    "    attribute: attribute/feature to split upon\n",
    "    value: value of that attribute to split upon\n",
    "\n",
    "    return: splitted data(Input and output)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Split the data based on a particular value of a particular attribute. \n",
    "    # You may use masking as a tool to split the data.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, data=None, feature_value=None,target=None,child=None,result=None,threshold=0):\n",
    "        self.data = data  # data corresponding to the node [matrix]\n",
    "        self.target = target  # y data\n",
    "        self.children = child  # child names & objects\n",
    "        self.feature = feature_value  # value at node\n",
    "        self.result = result\n",
    "        self.threshold=threshold\n",
    "\n",
    "    def plot(self, level=0):\n",
    "        indent = \"  \" * level\n",
    "        print(f\"{indent}|- {self.feature}: {self.result}\")\n",
    "        if self.children is not None:\n",
    "            for child in self.children:\n",
    "                self.children[child].plot(level + 1)\n",
    "\n",
    "class DecisionTree:\n",
    "    # criterion won't be used for regression\n",
    "    criterion: Literal[\"information_gain\", \"gini_index\"]\n",
    "    max_depth: int  # The maximum depth the tree can grow to\n",
    "    root = None\n",
    "\n",
    "    def __init__(self, criterion, max_depth=5):\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    # def fit_DI_DO(self, data_frame: pd.DataFrame, target: pd.Series, depth=0,criterion=\"information_gain\"):\n",
    "    #     return self.id3(data_frame,target,depth)\n",
    "\n",
    "    def id3(self,data_frame: pd.DataFrame, target: pd.Series, depth=0,criterion=\"information_gain\"):\n",
    "            # If all instances have the same target value, create a leaf node\n",
    "\n",
    "            #no split of data set is possible when we have 2 or lesser samples\n",
    "            if target.size <= 2:\n",
    "                mean_value = target.mean()\n",
    "                return Node(result=mean_value)\n",
    "\n",
    "            # If there are no more features to split on, create a leaf node with the majority class\n",
    "            if len(list(data_frame.columns)) == 0:\n",
    "                mean_value = target.mean()\n",
    "                return Node(result=mean_value)\n",
    "\n",
    "            # If maximum depth is reached, create a leaf node with the majority class\n",
    "            if self.max_depth is not None and depth == self.max_depth:\n",
    "                mean_value = target.mean()\n",
    "                return Node(result=mean_value)\n",
    "\n",
    "            # Choose the best feature to split on based on information gain considering only single level in tree\n",
    "            best_attribute=opt_split_attribute(data_frame,target,criterion=criterion)\n",
    "            root=Node(data=data_frame,target=target,feature_value=best_attribute[0],child={},threshold=best_attribute[1])\n",
    "\n",
    "            print(\"donee \")\n",
    "            \n",
    "            r1_x=data_frame[data_frame[best_attribute[0]] < best_attribute[1]]\n",
    "            r1_y=target[data_frame[best_attribute[0]] < best_attribute[1]]\n",
    "\n",
    "            r2_x=data_frame[data_frame[best_attribute[0]] >= best_attribute[1]]\n",
    "            r2_y=target[data_frame[best_attribute[0]] >= best_attribute[1]]\n",
    "\n",
    "            if r1_y.size > 0 :\n",
    "                root.children[0]=self.id3(r1_x, r1_x,depth=depth+1)\n",
    "\n",
    "            if r2_y.size > 0 :\n",
    "                root.children[1]=self.id3(r2_x, r2_y,depth=depth+1)\n",
    "\n",
    "            return root\n",
    "\n",
    "    def predict_multiway(self,tree :Node, sample):\n",
    "        # Reached a leaf node which contains the result value, return the result\n",
    "        if tree.result is not None:\n",
    "            return tree.result\n",
    "\n",
    "        # Not a leaf node, continue traversing the tree\n",
    "        feature_value = sample[tree.feature]\n",
    "\n",
    "        if feature_value < tree.threshold:\n",
    "            child_node = tree.children[0]\n",
    "        else:\n",
    "            child_node = tree.children[1]\n",
    "        return self.predict_multiway(child_node, sample)\n",
    "\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series):\n",
    "        # if check_ifreal(y):\n",
    "    #     return self.id3(X,y,depth=0,criterion=\"MSE\")     \n",
    "    # else:\n",
    "    #      return self.id3(X,y,depth=0,criterion=self.criterion)  \n",
    "        return self.id3(X,y,depth=0,criterion=self.criterion)   \n",
    "\n",
    "    def predict(self, tree,X):\n",
    "        return self.predict_multiway(tree,X)\n",
    "\n",
    "    def plot(self):\n",
    "        \"\"\"\n",
    "        Function to plot the tree\n",
    "\n",
    "        Output Example:\n",
    "        ?(X1 > 4)\n",
    "            Y: ?(X2 > 7)\n",
    "                Y: Class A\n",
    "                N: Class B\n",
    "            N: Class C\n",
    "        Where Y => Yes and N => No\n",
    "        \"\"\"\n",
    "        if self.root is not None:\n",
    "            self.root.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.12445271335215562\n",
      "loss 0.12501209695213814\n",
      "loss 0.14418203231899676\n",
      "loss 0.10293405389244553\n",
      "loss 0.12445271335215562\n",
      "loss 0.08836298040712562\n",
      "loss 0.12445271335215562\n",
      "loss 0.1131176683324782\n",
      "loss 0.1131176683324782\n",
      "loss 0.1652900748199381\n",
      "loss 0.11567458587902008\n",
      "loss 0.14418203231899676\n",
      "loss 0.1519385908516112\n",
      "loss 0.1652900748199381\n",
      "loss 0.11881143078107803\n",
      "loss 0.12445271335215562\n",
      "loss 0.1131176683324782\n",
      "loss 0.09454284184488979\n",
      "loss 0.07286213791449493\n",
      "loss 0.12501209695213814\n",
      "loss 0.08836298040712562\n",
      "loss 0.08836298040712562\n",
      "loss 0.1652900748199381\n",
      "loss 0.0799587439622042\n",
      "loss 0.1255466669200042\n",
      "loss 0.1131176683324782\n",
      "loss 0.1519385908516112\n",
      "loss 0.1652900748199381\n",
      "loss 0.17616604593053273\n",
      "loss 0.1742646505443559\n",
      "loss 0.1742646505443559\n",
      "loss 0.1742646505443559\n",
      "loss 0.14662203506784777\n",
      "loss 0.20991259267443535\n",
      "loss 0.12836417409296108\n",
      "loss 0.12209711992657662\n",
      "loss 0.14553139436703202\n",
      "loss 0.1742646505443559\n",
      "loss 0.1742646505443559\n",
      "loss 0.11664765468416241\n",
      "loss 0.17616604593053273\n",
      "loss 0.17616604593053273\n",
      "loss 0.14457095540345782\n",
      "loss 0.17616604593053273\n",
      "loss 0.16899782743855524\n",
      "loss 0.17616604593053273\n",
      "loss 0.1155858021875128\n",
      "loss 0.17616604593053273\n",
      "loss 0.16899782743855524\n",
      "loss 0.11488054770794318\n",
      "loss 0.1742646505443559\n",
      "loss 0.42123956659389156\n",
      "loss 0.17213427316596447\n",
      "loss 0.14254565009340445\n",
      "loss 0.14457095540345782\n",
      "loss 0.11714686508388074\n",
      "loss 0.08811568640615577\n",
      "loss 0.11173234511474292\n",
      "loss 0.1097325132937979\n",
      "loss 0.1097325132937979\n",
      "loss 0.1097325132937979\n",
      "loss 0.1951231431710739\n",
      "loss 0.1951231431710739\n",
      "loss 0.08811568640615577\n",
      "loss 0.1097325132937979\n",
      "loss 0.1491250270352319\n",
      "loss 0.1951231431710739\n",
      "loss 0.1186605786689676\n",
      "loss 0.08811568640615577\n",
      "loss 0.11715057968897888\n",
      "loss 0.08811568640615577\n",
      "loss 0.13847347263580317\n",
      "loss 0.11715057968897888\n",
      "loss 0.11715057968897888\n",
      "loss 0.1186605786689676\n",
      "loss 0.1186605786689676\n",
      "loss 0.10763857244858263\n",
      "loss 0.1491250270352319\n",
      "loss 0.10763857244858263\n",
      "loss 0.08811568640615577\n",
      "loss 0.1951231431710739\n",
      "loss 0.17143853129517575\n",
      "loss 0.1951231431710739\n",
      "loss 0.10093856225492873\n",
      "loss 0.10800626473451279\n",
      "loss 0.1503496912906482\n",
      "loss 0.10800626473451279\n",
      "loss 0.14203500349913478\n",
      "loss 0.11959826551998277\n",
      "loss 0.10800626473451279\n",
      "loss 0.11959826551998277\n",
      "loss 0.11469101788385178\n",
      "loss 0.20306841838680356\n",
      "loss 0.17131575234644256\n",
      "loss 0.11480391780563581\n",
      "loss 0.11480391780563581\n",
      "loss 0.07305150442012866\n",
      "loss 0.07305150442012866\n",
      "loss 0.11592791927678742\n",
      "loss 0.13280587513882297\n",
      "loss 0.11480391780563581\n",
      "loss 0.11592791927678742\n",
      "loss 0.11480391780563581\n",
      "loss 0.11592791927678742\n",
      "loss 0.0977739426882508\n",
      "loss 0.0307925567678107\n",
      "loss 0.11592791927678742\n",
      "loss 0.12942715292844004\n",
      "loss 0.12906580650029575\n",
      "loss 0.09861463040706996\n",
      "loss 0.20306841838680356\n",
      "loss 0.16503509502533176\n",
      "loss 0.11410328464107768\n",
      "loss 0.1321168412956287\n",
      "loss 0.14346158570037587\n",
      "loss 0.10711384583163824\n",
      "loss 0.1455138025108249\n",
      "loss 0.13422419235029545\n",
      "loss 0.11410328464107768\n",
      "loss 0.1321168412956287\n",
      "loss 0.10733563291510076\n",
      "loss 0.029015698855037134\n",
      "loss 0.4188170699367729\n",
      "loss 0.12151381184090845\n",
      "loss 0.1321168412956287\n",
      "loss 0.030371867576210798\n",
      "loss 0.13416007257563786\n",
      "loss 0.12416795725054508\n",
      "loss 0.12151381184090845\n",
      "loss 0.10733563291510076\n",
      "loss 0.11641159079381491\n",
      "loss 0.12416795725054508\n",
      "loss 0.12416795725054508\n",
      "loss 0.1455138025108249\n",
      "loss 0.2128190805534376\n",
      "loss 0.12151381184090845\n",
      "loss 0.11397927660982204\n",
      "loss 0.14346158570037587\n",
      "loss 0.10711384583163824\n",
      "loss 0.1455138025108249\n",
      "donee \n",
      "loss 0    0.023298\n",
      "1    0.069388\n",
      "2    0.067109\n",
      "3    0.118486\n",
      "4    0.099886\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Himani\\AppData\\Local\\Temp\\ipykernel_17940\\4265177746.py:31: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  r1_y=Y[X[feature_idx] < threshold]\n",
      "C:\\Users\\Himani\\AppData\\Local\\Temp\\ipykernel_17940\\4265177746.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  r2_y=Y[X[feature_idx] > threshold]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[164], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# for criteria in [\"information_gain\", \"gini_index\"]:\u001b[39;00m\n\u001b[0;32m      8\u001b[0m tree \u001b[38;5;241m=\u001b[39m DecisionTree(criterion\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minformation_gain\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Split based on Inf. Gain\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m root\u001b[38;5;241m=\u001b[39m\u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[163], line 88\u001b[0m, in \u001b[0;36mDecisionTree.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: pd\u001b[38;5;241m.\u001b[39mDataFrame, y: pd\u001b[38;5;241m.\u001b[39mSeries):\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;66;03m# if check_ifreal(y):\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m#     return self.id3(X,y,depth=0,criterion=\"MSE\")     \u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m#      return self.id3(X,y,depth=0,criterion=self.criterion)  \u001b[39;00m\n\u001b[1;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[163], line 61\u001b[0m, in \u001b[0;36mDecisionTree.id3\u001b[1;34m(self, data_frame, target, depth, criterion)\u001b[0m\n\u001b[0;32m     58\u001b[0m r2_y\u001b[38;5;241m=\u001b[39mtarget[data_frame[best_attribute[\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m best_attribute[\u001b[38;5;241m1\u001b[39m]]\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m r1_y\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m :\n\u001b[1;32m---> 61\u001b[0m     root\u001b[38;5;241m.\u001b[39mchildren[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr1_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr1_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m r2_y\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m :\n\u001b[0;32m     64\u001b[0m     root\u001b[38;5;241m.\u001b[39mchildren[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid3(r2_x, r2_y,depth\u001b[38;5;241m=\u001b[39mdepth\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[163], line 49\u001b[0m, in \u001b[0;36mDecisionTree.id3\u001b[1;34m(self, data_frame, target, depth, criterion)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Node(result\u001b[38;5;241m=\u001b[39mmean_value)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Choose the best feature to split on based on information gain considering only single level in tree\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m best_attribute\u001b[38;5;241m=\u001b[39m\u001b[43mopt_split_attribute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_frame\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m root\u001b[38;5;241m=\u001b[39mNode(data\u001b[38;5;241m=\u001b[39mdata_frame,target\u001b[38;5;241m=\u001b[39mtarget,feature_value\u001b[38;5;241m=\u001b[39mbest_attribute[\u001b[38;5;241m0\u001b[39m],child\u001b[38;5;241m=\u001b[39m{},threshold\u001b[38;5;241m=\u001b[39mbest_attribute[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdonee \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[162], line 71\u001b[0m, in \u001b[0;36mopt_split_attribute\u001b[1;34m(X, Y, criterion)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column_name \u001b[38;5;129;01min\u001b[39;00m X\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m     70\u001b[0m     X\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39mcolumn_name)\n\u001b[1;32m---> 71\u001b[0m     new_info_gain\u001b[38;5;241m=\u001b[39m\u001b[43minformation_gain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcolumn_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m new_info_gain[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m<\u001b[39m max_info_gain:\n\u001b[0;32m     73\u001b[0m         max_info_gain\u001b[38;5;241m=\u001b[39mnew_info_gain\n",
      "Cell \u001b[1;32mIn[162], line 43\u001b[0m, in \u001b[0;36minformation_gain\u001b[1;34m(attr, Y, feature_idx)\u001b[0m\n\u001b[0;32m     39\u001b[0m new_loss\u001b[38;5;241m=\u001b[39mMSE(r1_y,r1_y\u001b[38;5;241m.\u001b[39mmean())\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(r1_x) \u001b[38;5;241m+\u001b[39m \\\n\u001b[0;32m     40\u001b[0m         MSE(r2_y, r2_y\u001b[38;5;241m.\u001b[39mmean()) \u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(r2_x)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m , new_loss)\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mnew_loss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m:\n\u001b[0;32m     44\u001b[0m     loss\u001b[38;5;241m=\u001b[39mnew_loss\n\u001b[0;32m     45\u001b[0m     final_thresholder\u001b[38;5;241m=\u001b[39mthreshold\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:1527\u001b[0m, in \u001b[0;36mNDFrame.__nonzero__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/anaconda3/lib/site-packages/pandas/core/generic.py?line=1524'>1525</a>\u001b[0m \u001b[39m@final\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/anaconda3/lib/site-packages/pandas/core/generic.py?line=1525'>1526</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__nonzero__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m NoReturn:\n\u001b[1;32m-> <a href='file:///c%3A/ProgramData/anaconda3/lib/site-packages/pandas/core/generic.py?line=1526'>1527</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/anaconda3/lib/site-packages/pandas/core/generic.py?line=1527'>1528</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe truth value of a \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m is ambiguous. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/anaconda3/lib/site-packages/pandas/core/generic.py?line=1528'>1529</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUse a.empty, a.bool(), a.item(), a.any() or a.all().\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/anaconda3/lib/site-packages/pandas/core/generic.py?line=1529'>1530</a>\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "N = 30\n",
    "P = 5\n",
    "X = pd.DataFrame(np.random.randn(N, P))\n",
    "y = pd.Series(np.random.randn(N))\n",
    "\n",
    "\n",
    "# for criteria in [\"information_gain\", \"gini_index\"]:\n",
    "tree = DecisionTree(criterion=\"information_gain\")  # Split based on Inf. Gain\n",
    "root=tree.fit(X, y)\n",
    "# y_hat = root.predict(X)\n",
    "# tree.plot()\n",
    "# print(\"Criteria :\", \"information_gain\")\n",
    "# print(\"RMSE: \", rmse(y_hat, y))\n",
    "# print(\"MAE: \", mae(y_hat, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3de59ade2ee4eabf8d6554090db31bdd94608df00f05391c2d316a7da62ee3f6"
  },
  "kernelspec": {
   "display_name": "Python 3.10.9 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
